= Kafka Fundamentals Brain Teasers: M5: Integrating Kafka into Your Environment
:imagesdir: ./new-images-copy
:source-highlighter: rouge
:icons: font


<<<

== Overview

Key concepts from Module 5 are:

* Kafka Connect
* Connectors
* Worker groups
* Confluent REST Proxy
* Data compatibility
* Confluent Schema Registry
* Schema evolution
* ksqlDB
* Kafka Streams
* Comparing ksqlDB, Kafka Streams, and the Producer/Consumer APIs




Here’s the https://forms.gle/hGP6uvcwh9m925SQ6[quick quiz on Module 5] from the Online Talk Series.

Before we proceed to problem #4, let’s dig into one of the quiz questions and the correct answer as shown below in bold text.

Suppose we have an Avro schema to represent a `song`. It consists of the following fields and types:

* `title`, `string`
* `artist`, `string`
* `year`, `int`
* `song_id`, `int`

Suppose, additionally, we are using Schema Registry, and this schema is totally new. Then:

1. Suppose a producer wants to produce a message whose value is of type `song` to the topic `songs-topic`. We are using the Avro serializer. How does the producer know what schema ID to use? 

* Producer assigns schema ID
* Schema ID found in producer's cache
* *Schema ID generated by Schema Registry and sent to producer*
* Schema ID randomly generated

2. Oops, we realized we also wanted to include a field for genre in the `song` schema, so we make a new version of the schema. Will this version use the same schema ID or a different one? Explain. 

* Same
* *Different*
* It depends

3. consumer, configured to use the Avro deserializer for values, reads from the topic `songs-topic` for the first time. How does it know how to deserialize the value of the message correctly? 

* Consumer can pull off appended schema ID and look up schema in Schema Registry
* *Consumer can pull off prepended schema ID and look up schema in Schema Registry*
* Consumer can pull off appended schema ID and look up schema by polling Kafka again
* Consumer can pull off prepended schema ID and look up schema by polling Kafka again

Question A above is about https://docs.confluent.io/platform/current/schema-registry/index.html[Confluent Schema Registry]. It’s about populating a topic for songs that our jukeboxes might play. There’s no built-in data type for a `song`, of course, we have to define one. We could use a JSON schema to represent the fields above and then use Avro, along with Schema Registry, to perform the heavy lifting. Producers must serialize messages to send them across the “wire” and for them to be stored in Kafka. Avro can do all of the serialization work, as long as it has the `song` schema. From there, the schema used to serialize songs has to become associated with each song. Imagine in our jukebox application, we have thousands of songs. If we sent the `song` schema along with every one of them, that’s thousands of extra copies costing extra bandwidth of network traffic. The solution: Schema Registry.

When we have a producer wired up to use Schema Registry and Avro and we first try to send a message whose value uses the `song` schema, the producer sends that schema to the Schema Registry. The Schema Registry doesn’t know the schema, so it registers the schema, and sends back a new schema ID to represent the `song` schema. Let’s say that schema ID is 72. Avro serializes the value and prepends the schema ID 72 to the message. 

For Question B above, when we add a new `genre` field to the `song` schema, we are effectively creating Version 2 of the `song` schema. When we use this schema for the first time, the Schema Registry will register this as a _new version_ of the `song` schema, so it will get a different schema ID. Let’s say it’s 105. In short, the Schema Registry stores a versioned history of all schemas and a schema ID uniquely identifies a _version of_ a schema. 

Per Question C, when a consumer reads a message whose value had been prepended with schema ID 105, the consumer will pull off the prepended schema ID 105, ask the Schema Registry for the corresponding schema, and get Version 2 of the `song` schema (the one with `genre` too), and use Avro to deserialize. 

This question is here because it provides some additional context to the problem we are about to cover. The problem below is designed to dive deeper into the digital jukebox application discussed earlier.


ifdef::artifact-type[]
---
guide

 
endif::artifact-type[]


<<<

== Problem #5A: Back to the Jukebox with ksqlDB

Suppose we are building a jukebox app and there is also a topic `plays-topic`. Many messages have been produced to it. For each message:

* The key is a `song_id`, formatted as a `string` again
* The value of each message is a JSON-formatted string `song_play`, containing a timestamp when the song was played and `jukebox_id`, identifying which jukebox played it

Suppose we have ksqlDB set up with a `TABLE` called songs, sourced from the topic `songs-topic`, and a `STREAM` called `song_plays`, sourced from the topic plays-topic. Write a ksqlDB statement that joins together each `song_play` with the corresponding title, artist, and year and creates a `STREAM` called `song_plays_enriched`. 




ifdef::artifact-type[]
---
guide

 
endif::artifact-type[]

<<<



== Problem #5B: Figure out the Mystery Query!

Suppose we are building a jukebox app and there is also a topic `plays-topic`. Many messages have been produced to it. For each message:

* The key is a `song_id`, formatted as a string again
* The value of each message is a JSON-formatted string `song_play`, containing a timestamp when the song was played and `jukebox_id`, identifying which jukebox played it

Suppose we have ksqlDB set up with a `TABLE` called `songs`, sourced from the topic `songs-topic`, and a `STREAM` called `song_plays`, sourced from the topic `plays-topic`. Here's a different ksqlDB statement that illustrates the power of ksqlDB:

``` 
CREATE TABLE mystery AS 
   SELECT artist, count(*) 
   FROM song_plays_enriched 
   WINDOW TUMBLING (SIZE 1 HOUR) 
   GROUP BY artist 
   HAVING count(*) >= 5 
   EMIT CHANGES; 
```

What does this do?


ifdef::artifact-type[]
---
guide


endif::artifact-type[]




<<<



== Problem #5C - Extra Problem: Getting Song Data to the Jukebox 

Continuing off problem #5B, all of the song data for all of the songs offered on this company's jukeboxes lives on a relational database. How might you get all of that data into the Kafka cluster in the first place?


ifdef::artifact-type[]
---
guide


endif::artifact-type[]


